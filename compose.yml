version: '3.8'

networks:
  ros_net:
    driver: bridge

services:
  vv_inf:
    build:
      context: .
      dockerfile: Dockerfile
      shm_size: '4gb'  # –í–∞–∂–Ω–æ –¥–ª—è PyTorch DataLoader
    image: vv_inf:latest
    container_name: vv_inf
    hostname: vv_inf
    stdin_open: true
    tty: true
    
    # Network
    networks:
      - ros_net

    privileged: true
    
    # Shared memory –¥–ª—è PyTorch
    shm_size: '4gb'
    
    # Environment
    env_file:
      - .env
    environment:
      - DISPLAY=${DISPLAY}
      - ROS_DOMAIN_ID=21
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      # –î–ª—è OpenCV GUI
      - QT_X11_NO_MITSHM=1
      - LIBGL_ALWAYS_INDIRECT=0
      # Python optimization
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
    
    # GPU support
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # Volumes
    volumes:
      # Hardware access
      - /dev:/dev
      # X11 for GUI
      - /tmp/.X11-unix:/tmp/.X11-unix:rw
      # Source code (–¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
      - ./src/:/root/ros_ws/src/:rw
    
    # Command –¥–ª—è dev —Ä–µ–∂–∏–º–∞
    command:
      - bash
      - -c
      - |
        # Source workspace
        source /root/ros_ws/install/setup.bash
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
        echo "üéÆ Checking GPU availability..."
        python3 -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\"}')" || true
        
        
        # Keep container running
        tail -f /dev/null
    
    restart: unless-stopped
    
  